{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 250)\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from routingpy.routers import MapboxOSRM\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import shapely.geometry\n",
    "import shapely.wkt\n",
    "from shapely.wkt import loads\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from palettable.colorbrewer.sequential import Reds_9\n",
    "\n",
    "from lonboard import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in our csv of Dallas Trauma Centers\n",
    "df = pd.read_csv('data/raw/dallas_hospitals.csv')\n",
    "\n",
    "#convert the DataFrame to a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['long'], df['lat'], crs='EPSG:4326'))\n",
    "gdf.explore(tiles='CartoDBDarkMatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read txt file with our API key\n",
    "with open('mbx_api_key.txt') as f:\n",
    "    api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was mostly borrowed (*cough*stolen*cough*) from Kyle Walker here: https://walker-data.com/posts/python-isochrones/\n",
    "\n",
    "#Get API key we just read in from our text file\n",
    "mb = MapboxOSRM(api_key = api_key)\n",
    "\n",
    "def mb_isochrone(gdf: gpd.GeoDataFrame, \n",
    "                 time = [5, 10, 15], #must be either 5, 10 or 15 minutes\n",
    "                 profile = \"driving-traffic\"): #use driving with traffic profile by default\n",
    "\n",
    "    #get our lat/long coordinates as a list of lists\n",
    "    coordinates = gdf[['long', 'lat']].values.tolist()\n",
    "\n",
    "    # Create an empty list to store the isochrone geometries\n",
    "    isochrone_shapes = []\n",
    "\n",
    "    #create a list for the size of the isochrone we want to generate, expressed in minutes of time\n",
    "    if type(time) is not list:\n",
    "        time = [time]\n",
    "\n",
    "    # API requires seconds so multiplify by 60\n",
    "    time_seconds = [60 * x for x in time]\n",
    "\n",
    "    # Given the way that routingpy works, we need to iterate through the list of \n",
    "    # coordinate pairs, then iterate through the object returned and extract the \n",
    "    # isochrone geometries.  \n",
    "    for c in coordinates:\n",
    "        iso_request = mb.isochrones(locations = c, #lat/long pair\n",
    "                                    profile = profile, #driving with traffic\n",
    "                                    intervals = time_seconds, #the time period, expressed in seconds\n",
    "                                    polygons = \"true\") #return polygons\n",
    "        \n",
    "        #returns the polygons for the isochrone\n",
    "        for i in iso_request:\n",
    "            iso_geom = Polygon(i.geometry[0])\n",
    "            isochrone_shapes.append(iso_geom)\n",
    "\n",
    "    # Rebuild the dataset but with isochrone geometries\n",
    "    df_values = gdf.drop(columns = ['geometry', 'long', 'lat'])\n",
    "\n",
    "    time_col = time * len(df_values)\n",
    "\n",
    "    # We'll need to repeat the dataframe to account for multiple time intervals\n",
    "    df_values_rep = pd.DataFrame(np.repeat(df_values.values, len(time_seconds), axis = 0))\n",
    "    df_values_rep.columns = df_values.columns\n",
    "\n",
    "    isochrone_gdf = gpd.GeoDataFrame(\n",
    "        data = df_values_rep,\n",
    "        geometry = isochrone_shapes,\n",
    "        crs = 4326\n",
    "    )\n",
    "\n",
    "    isochrone_gdf['time'] = time_col\n",
    "\n",
    "    # We are sorting the dataframe in descending order of time to improve visualization\n",
    "    # (the smallest isochrones should go on top, which means they are plotted last)\n",
    "    isochrone_gdf = isochrone_gdf.sort_values('time', ascending = False)\n",
    "\n",
    "    return(isochrone_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty GeoDataFrame to store isochrones in\n",
    "gdfx2 = gpd.GeoDataFrame()\n",
    "\n",
    "#add isochrones iterativley to gdf\n",
    "for i in [5, 10, 15]:\n",
    "    gdfx = mb_isochrone(gdf, time = i)\n",
    "    gdfx2 = pd.concat([gdfx, gdfx2])\n",
    "\n",
    "gdfx2 = gdfx2.reset_index(drop=True).copy()\n",
    "\n",
    "gdfx2.explore('time', cmap='Reds', tiles='CartoDBDarkMatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfx2.loc[((gdfx2['aha_id'] == 6741050) & (gdfx2['time'] == 15))].explore(tiles='CartoDBDarkMatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column based on how far many minutes away the isochrone is ie. geometry_5, geometry_10 etc.\n",
    "for i in list(gdfx2['time'].unique()):\n",
    "    gdfx2.loc[gdfx2['time'] == i, f'geometry_{i}'] =  gdfx2['geometry']\n",
    "\n",
    "#reset index\n",
    "gdfx2 = gdfx2.reset_index(drop=True).copy()\n",
    "\n",
    "#make a copy of the original gdf\n",
    "gdfx3 = gdfx2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Back to my code â€” loop through the isochrones for each trauma center\n",
    "for center in list(gdfx3['tcn'].unique()):\n",
    "    #loop through each time period for that trauma centers' isochrones, in increments of 3 mins from 3 to 27 mins.\n",
    "    for i in range(5, 15, 5): \n",
    "        #take the area covered by the smaller isochrone and remove it from the next biggest isochrone to avoid overlap, ie. 6min - 3min\n",
    "        big  = gdfx3.loc[((gdfx3['tcn'] == center) & (gdfx3[f'geometry_{i+5}'].notna())), f'geometry_{i + 5}'].iloc[0]\n",
    "        small = gdfx3.loc[((gdfx3['tcn'] == center) & (gdfx3[f'geometry_{i}'].notna())), f'geometry_{i}'].iloc[0]\n",
    "        gdfx3.loc[((gdfx3['tcn'] == center) & (gdfx3['time'] == (i + 5))), 'geometry'] = big.difference(small)\n",
    "\n",
    "\n",
    "#make a copy of the output, in case we want to re-run and start from here.\n",
    "gdfx4 = gdfx3.reset_index(drop=True).copy()\n",
    "\n",
    "#now delete those columns we made earlier for each time period.\n",
    "for i in range(5, 20, 5):\n",
    "    gdfx4 = gdfx4.drop([f'geometry_{i}'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfx4.loc[((gdfx4['aha_id'] == 6741050) & (gdfx2['time'] == 15))].explore(tiles='CartoDBDarkMatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First trace the boundaries for all the isochrone polygon intersections\n",
    "lines = gdfx4.apply(lambda x: x[\"geometry\"].boundary, axis=1).to_list() #Create a list of all polygon boundaries, both outer and inner rings\n",
    "\n",
    "# Combine all of those lines into a big multiline shape\n",
    "noded_lines = shapely.ops.unary_union(lines) \n",
    "\n",
    "#Convert that big multiline shape back into component polygons\n",
    "noded_lines_singleparts = [x for x in noded_lines.geoms] #First convert to a list of LineStrings\n",
    "new_polys = list(shapely.polygonize(noded_lines_singleparts).geoms) #Then make polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer attributes from original polygons to newly formed polygons, by using points in the polygons\n",
    "\n",
    "#Create point-in-polygon features by creating a dataframe from the list of polygons  \n",
    "new_polys = gpd.GeoDataFrame(geometry=new_polys, crs=gdfx4.crs) \n",
    "\n",
    "#Create a copy of it to use to create points in polygons\n",
    "new_polys_point = new_polys.copy() \n",
    "new_polys_point[\"geometry\"] = new_polys.geometry.representative_point()\n",
    "\n",
    "#Overlay points with original polygons, transferring the attributes (ie. minutes from trauma center)\n",
    "points_with_attributes = gpd.sjoin(left_df=new_polys_point, right_df=gdfx4, how=\"left\", predicate=\"within\")\n",
    "\n",
    "#Join the points to the newly formed polygons, transferring the attributes (minutes from trauma center):\n",
    "del(points_with_attributes[\"index_right\"]) #delete index_right col to make join wont fail\n",
    "\n",
    "new_polys_with_attributes = gpd.sjoin(left_df=new_polys, right_df=points_with_attributes,\n",
    "                                      how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the polygon table by geometry and minutes from the trauma center.\n",
    "new_polys_with_attributes[\"wkt\"] = new_polys_with_attributes.apply(lambda x: x['geometry'].wkt, axis=1)\n",
    "new_polys_with_attributes = new_polys_with_attributes.sort_values(by=[\"wkt\", \"time\"], ascending=[True, False])\n",
    "new_polys_with_attributes = new_polys_with_attributes.drop_duplicates(subset=\"wkt\", keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to only columns we need\n",
    "gdfx5 = new_polys_with_attributes[['tcn', 'time', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdfx5.explore('time', cmap='Reds', tiles='CartoDBDarkMatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to most optimized format\n",
    "gdfx5.to_parquet('data/created/bleeding_out_nicar_demo_isochrones.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
